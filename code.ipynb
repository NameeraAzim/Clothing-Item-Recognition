{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Project 3"
      ],
      "metadata": {
        "id": "ytzFel1JLCaS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the required packages\n",
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "import cv2\n",
        "import numpy as np\n",
        "import random\n",
        "import torchvision.transforms.functional as F\n",
        "from torchvision.io import read_image\n",
        "from torchvision.ops.boxes import masks_to_boxes\n",
        "from PIL import Image\n",
        "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
        "from torchvision.models.detection.mask_rcnn import MaskRCNNPredictor\n",
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "gJXPoMhi0HWP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Mount Google Drive. This notebook was prepared on Google Colab. The package for project 3 was uploaded to Google Drive.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bUilkq1gxRZW",
        "outputId": "58016d31-2b37-4f95-fee7-0d5782c0805a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GPU will be used throughout this notebook if available. If not, CPU will be used.\n",
        "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')"
      ],
      "metadata": {
        "id": "oR0IHfHL3vH_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loading"
      ],
      "metadata": {
        "id": "ZwMZC_jz0DxC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize the list to hold all images for training.\n",
        "imgs_training = []\n",
        "# List of names of all image files.\n",
        "all_image_files = list(sorted(os.listdir(os.path.join(\"drive\", \"MyDrive\", \"project3\", \"images\"))))\n",
        "# 50% of the image files are selected for training.\n",
        "image_files = all_image_files[:300]\n",
        "\n",
        "# Iterate over each image to load and store in the list.\n",
        "for image_file in image_files:\n",
        "    img = cv2.imread(os.path.join(\"drive\", \"MyDrive\", \"project3\", \"images\", image_file))\n",
        "    # The pixel values are normalized to be within the range [0,1].\n",
        "    img_tensor = torch.as_tensor(img, dtype=torch.float32) / 255.0\n",
        "    imgs_training.append(img_tensor)\n",
        "\n",
        "# Stack the list of tensors into a single tensor.\n",
        "imgs_training = torch.stack(imgs_training, 0)\n",
        "# Axes are swapped to comply with the input requirements of the model.\n",
        "imgs_training = imgs_training.swapaxes(1, 3).swapaxes(2, 3)"
      ],
      "metadata": {
        "id": "hgwKlJQJMIS0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of names of all mask files.\n",
        "all_mask_files_person = sorted([file for file in os.listdir(os.path.join(\"drive\", \"MyDrive\", \"project3\", \"labels\")) if \"_person\" in file])\n",
        "# 50% of the mask files are selected for training.\n",
        "mask_files_person = all_mask_files_person[:300]\n",
        "# Initialize the list to hold all targets for training.\n",
        "targets_person_training = []\n",
        "\n",
        "# Iterate over each mask file to load and store in the list.\n",
        "for mask_file in mask_files_person:\n",
        "    mask = read_image(os.path.join(\"drive\", \"MyDrive\", \"project3\", \"labels\", mask_file))\n",
        "    target = {}\n",
        "    # Unique object IDs, i.e. labels, in the mask.\n",
        "    obj_ids = torch.unique(mask)\n",
        "    # 0 refers to the background and does not need to be included in the target.\n",
        "    obj_ids = obj_ids[1:]\n",
        "\n",
        "    # Create boolean masks indicating where elements of the `mask` tensor match any of the object IDs in `obj_ids`.\n",
        "    # Each element of `target_masks` corresponds to whether the corresponding element in `mask` matches any object ID.\n",
        "    target_masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "    target[\"masks\"] = torch.as_tensor(target_masks, dtype=torch.uint8)\n",
        "\n",
        "    # The bounding boxes of the objects in the mask.\n",
        "    target[\"boxes\"] = masks_to_boxes(target_masks)\n",
        "\n",
        "    # Unique labels in the mask without the background.\n",
        "    labels = torch.unique(mask.flatten())\n",
        "    labels = labels[1:]\n",
        "    target[\"labels\"] = labels.to(torch.int64)\n",
        "\n",
        "    # Append the target to the list.\n",
        "    targets_person_training.append(target)"
      ],
      "metadata": {
        "id": "2IpjVmyOEPLK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# List of names of all mask files.\n",
        "all_mask_files_clothes = sorted([file for file in os.listdir(os.path.join(\"drive\", \"MyDrive\", \"project3\", \"labels\")) if \"_clothes\" in file])\n",
        "# 50% of the mask files are selected for training.\n",
        "mask_files_clothes = all_mask_files_clothes[:300]\n",
        "# Initialize the list to hold all targets for training.\n",
        "targets_clothes_training = []\n",
        "\n",
        "# Iterate over each mask file to load and store in the list.\n",
        "for mask_file in mask_files_clothes:\n",
        "    mask = read_image(os.path.join(\"drive\", \"MyDrive\", \"project3\", \"labels\", mask_file))\n",
        "    target = {}\n",
        "    # Unique object IDs, i.e. labels, in the mask.\n",
        "    obj_ids = torch.unique(mask)\n",
        "    # 0 refers to the background and does not need to be included in the target.\n",
        "    obj_ids = obj_ids[1:]\n",
        "\n",
        "    # Create boolean masks indicating where elements of the `mask` tensor match any of the object IDs in `obj_ids`.\n",
        "    # Each element of `target_masks` corresponds to whether the corresponding element in `mask` matches any object ID.\n",
        "    target_masks = (mask == obj_ids[:, None, None]).to(dtype=torch.uint8)\n",
        "    target[\"masks\"] = torch.as_tensor(target_masks, dtype=torch.uint8)\n",
        "\n",
        "    # The bounding boxes of the objects in the mask.\n",
        "    target[\"boxes\"] = masks_to_boxes(target_masks)\n",
        "\n",
        "    # Unique labels in the mask without the background.\n",
        "    labels = torch.unique(mask.flatten())\n",
        "    labels = labels[1:]\n",
        "    target[\"labels\"] = labels.to(torch.int64)\n",
        "\n",
        "    # Append the target to the list.\n",
        "    targets_clothes_training.append(target)"
      ],
      "metadata": {
        "id": "2LNzpqd_EMxF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pixel Labeling of a Person"
      ],
      "metadata": {
        "id": "z4tR2L3q0XdZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "xAHX7jal0tZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask R-CNN model from the torchvision library with pre-trained weights is loaded.\n",
        "model_person = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "# Number of input features for the classifier.\n",
        "in_features = model_person.roi_heads.box_predictor.cls_score.in_features\n",
        "# There are two classes in this model including 'background'.\n",
        "model_person.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=2)\n",
        "model_person.to(device)\n",
        "\n",
        "# Initialize an AdamW optimizer for updating the parameters of the model.\n",
        "optimizer = torch.optim.AdamW(params=model_person.parameters(), lr=0.0001)\n",
        "\n",
        "model_person.train()\n",
        "\n",
        "# List to store loss values to be used in displaying the learning curve.\n",
        "losses_list = []\n",
        "\n",
        "#Iterate through each epoch.\n",
        "for i in range(11): #INITIALLY SET TO 101\n",
        "    #Randomly select 10 images from the training set.\n",
        "    random_numbers = [random.randint(0, 299) for x in range(10)]\n",
        "    curr_imgs_training = [imgs_training[j] for j in random_numbers]\n",
        "    images = list(image.to(device) for image in curr_imgs_training)\n",
        "    curr_targets_person_training = [targets_person_training[j] for j in random_numbers]\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in curr_targets_person_training]\n",
        "\n",
        "    # Reset the gradients of the parameter to prepare for the next batch.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass to get the loss.\n",
        "    loss_dict = model_person(images, targets)\n",
        "\n",
        "    # Calculate the total loss.\n",
        "    losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "    # Backward pass to calculate the gradients.\n",
        "    losses.backward()\n",
        "\n",
        "    # Take a single optimization step to update the model parameters based on computed gradients and the optimizer's update rule.\n",
        "    optimizer.step()\n",
        "\n",
        "    losses_list.append(losses.item())\n",
        "    if i%10==0:\n",
        "        torch.save(model_person.state_dict(), str(i)+\".torch\")\n",
        "\n",
        "# Display the training loss curve\n",
        "plt.scatter(range(len(losses_list)), losses_list, marker='.')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs Epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "ZrywdzJamwM-",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 596
        },
        "outputId": "8bfde6f1-cb68-4f86-997e-73f4f620e8fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MaskRCNN_ResNet50_FPN_Weights.COCO_V1`. You can also use `weights=MaskRCNN_ResNet50_FPN_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n",
            "Downloading: \"https://download.pytorch.org/models/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\" to /root/.cache/torch/hub/checkpoints/maskrcnn_resnet50_fpn_coco-bf2d0c1e.pth\n",
            "100%|██████████| 170M/170M [00:00<00:00, 183MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation\n",
        "*Note:*\n",
        "\n",
        "*1. Predicted mask files are stored in a new folder under the \"project3\" folder called \"test\".*\n",
        "\n",
        "*2. Pixels of masks displayed in the report have been multiplied by 255 to make the different classes visible. The code below labels each pixel as 0 or 1 in align with the project requirement and the input required by evalseg.m.*"
      ],
      "metadata": {
        "id": "3asd1zYx0x-F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask_from_target(target_dict, output_path):\n",
        "    threshold = 0.75\n",
        "\n",
        "    # Filter predictions based on score threshold.\n",
        "    high_score_indices = target_dict[\"scores\"] > threshold\n",
        "    filtered_labels = target_dict[\"labels\"][high_score_indices].cpu().numpy()\n",
        "    filtered_masks = target_dict[\"masks\"][high_score_indices].cpu().numpy()\n",
        "\n",
        "    # To handle the edge-case where none of the masks cross the threshold when testing on unseen data,\n",
        "    # all available predicted masks are included.\n",
        "    if filtered_labels.size == 0:\n",
        "        filtered_labels = target_dict[\"labels\"].cpu().numpy()\n",
        "        filtered_masks = target_dict[\"masks\"].cpu().numpy()\n",
        "\n",
        "    # Create combined mask.\n",
        "    combined_mask = np.zeros_like(filtered_masks[0], dtype=np.uint8)\n",
        "    for label, mask in zip(filtered_labels, filtered_masks):\n",
        "        combined_mask[mask > 0] = label\n",
        "\n",
        "    # Save the mask image.\n",
        "    combined_mask_pil = Image.fromarray(combined_mask.squeeze(), mode='L')\n",
        "    combined_mask_pil.save(output_path)\n",
        "\n",
        "# Load the final stage of the trained model and set to evaluate.\n",
        "model_person.load_state_dict(torch.load(\"10.torch\")) #INITIALLY 100\n",
        "model_person.to(device)\n",
        "model_person.eval()\n",
        "\n",
        "# Select 10% of images for validation.\n",
        "image_validation_files = all_image_files[300:360]\n",
        "\n",
        "# Iterate through each image.\n",
        "for image in image_validation_files:\n",
        "    images = cv2.imread(os.path.join(\"drive\", \"MyDrive\", \"project3\", \"images\", image))\n",
        "    images = torch.as_tensor(images, dtype=torch.float32) / 255.0\n",
        "    images = images.permute(2,0,1).unsqueeze(0)\n",
        "    images = images.to(device)\n",
        "\n",
        "    # Get the prediction output of the model.\n",
        "    with torch.no_grad():\n",
        "        pred = model_person(images)\n",
        "\n",
        "    # Get the image number and create the appropriate file name and output path.\n",
        "    img_number = image[:-4]\n",
        "    output_path = os.path.join(\"drive\", \"MyDrive\", \"project3\", \"validation\", img_number + \"_person.png\")\n",
        "    create_mask_from_target(pred[0], output_path)"
      ],
      "metadata": {
        "id": "xyfo_m42T9Mi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test\n",
        "*Note:*\n",
        "\n",
        "*1. Predicted mask files are stored in a new folder under the \"project3\" folder called \"test\".*\n",
        "\n",
        "*2. Pixels of masks displayed in the report have been multiplied by 255 to make the different classes visible. The code below labels each pixel as 0 or 1 in align with the project requirement and the input required by evalseg.m.*"
      ],
      "metadata": {
        "id": "YIhO4k8K04Gh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask_from_target(target_dict, output_path):\n",
        "    threshold = 0.75\n",
        "\n",
        "    # Filter predictions based on score threshold.\n",
        "    high_score_indices = target_dict[\"scores\"] > threshold\n",
        "    filtered_labels = target_dict[\"labels\"][high_score_indices].cpu().numpy()\n",
        "    filtered_masks = target_dict[\"masks\"][high_score_indices].cpu().numpy()\n",
        "\n",
        "    # To handle the edge-case where none of the masks cross the threshold when testing on unseen data,\n",
        "    # all available predicted masks are included.\n",
        "    if filtered_labels.size == 0:\n",
        "        filtered_labels = target_dict[\"labels\"].cpu().numpy()\n",
        "        filtered_masks = target_dict[\"masks\"].cpu().numpy()\n",
        "\n",
        "    # Create combined mask.\n",
        "    combined_mask = np.zeros_like(filtered_masks[0], dtype=np.uint8)\n",
        "    for label, mask in zip(filtered_labels, filtered_masks):\n",
        "        combined_mask[mask > 0] = label\n",
        "\n",
        "    # Save the mask image.\n",
        "    combined_mask_pil = Image.fromarray(combined_mask.squeeze(), mode='L')\n",
        "    combined_mask_pil.save(output_path)\n",
        "\n",
        "# Load the final stage of the trained model and set to evaluate.\n",
        "model_person.load_state_dict(torch.load(\"10.torch\")) #INITIALLY 100\n",
        "model_person.to(device)\n",
        "model_person.eval()\n",
        "\n",
        "# Select 40% of images for testing.\n",
        "image_validation_files = all_image_files[360:]\n",
        "\n",
        "# Iterate through each image.\n",
        "for image in image_validation_files:\n",
        "    images = cv2.imread(os.path.join(\"drive\", \"MyDrive\", \"project3\", \"images\", image))\n",
        "    images = torch.as_tensor(images, dtype=torch.float32) / 255.0\n",
        "    images = images.permute(2,0,1).unsqueeze(0)\n",
        "    images = images.to(device)\n",
        "\n",
        "    # Get the prediction output of the model.\n",
        "    with torch.no_grad():\n",
        "        pred = model_person(images)\n",
        "\n",
        "    # Get the image number and create the appropriate file name and output path.\n",
        "    img_number = image[:-4]\n",
        "    output_path = os.path.join(\"drive\", \"MyDrive\", \"project3\", \"test\", img_number + \"_person.png\")\n",
        "    create_mask_from_target(pred[0], output_path)"
      ],
      "metadata": {
        "id": "HZcnLV83k4VF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pixel Labeling of Clothing Items"
      ],
      "metadata": {
        "id": "wnwORB8p0nHj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "vN4RoFqi08Rg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Mask R-CNN model from the torchvision library with pre-trained weights is loaded.\n",
        "model_clothes = torchvision.models.detection.maskrcnn_resnet50_fpn(pretrained=True)\n",
        "# Number of input features for the classifier.\n",
        "in_features = model_clothes.roi_heads.box_predictor.cls_score.in_features\n",
        "# There are seven classes in this model including 'background'.\n",
        "model_clothes.roi_heads.box_predictor = FastRCNNPredictor(in_features,num_classes=7)\n",
        "model_clothes.to(device)\n",
        "\n",
        "# Initialize an AdamW optimizer for updating the parameters of the model.\n",
        "optimizer = torch.optim.AdamW(params=model_clothes.parameters(), lr=0.0001)\n",
        "\n",
        "model_clothes.train()\n",
        "\n",
        "# List to store loss values to be used in displaying the learning curve.\n",
        "losses_list = []\n",
        "\n",
        "#Iterate through each epoch.\n",
        "for i in range(11): #INITIALLY SET TO 501\n",
        "    #Randomly select 10 images from the training set.\n",
        "    random_numbers = [random.randint(0, 299) for x in range(10)]\n",
        "    curr_imgs_training = [imgs_training[j] for j in random_numbers]\n",
        "    images = list(image.to(device) for image in curr_imgs_training)\n",
        "    curr_targets_clothes_training = [targets_clothes_training[j] for j in random_numbers]\n",
        "    targets = [{k: v.to(device) for k, v in t.items()} for t in curr_targets_clothes_training]\n",
        "\n",
        "    # Reset the gradients of the parameter to prepare for the next batch.\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    # Forward pass to get the loss.\n",
        "    loss_dict = model_clothes(images, targets)\n",
        "\n",
        "    # Calculate the total loss.\n",
        "    losses = sum(loss for loss in loss_dict.values())\n",
        "\n",
        "    # Backward pass to calculate the gradients.\n",
        "    losses.backward()\n",
        "\n",
        "    # Take a single optimization step to update the model parameters based on computed gradients and the optimizer's update rule.\n",
        "    optimizer.step()\n",
        "    losses_list.append(losses.item())\n",
        "    if i%10==0:\n",
        "        torch.save(model_clothes.state_dict(), str(i)+\".torch\")\n",
        "\n",
        "# Display the training loss curve\n",
        "plt.scatter(range(len(losses_list)), losses_list, marker='.')\n",
        "plt.xlabel('Epoch')\n",
        "plt.ylabel('Loss')\n",
        "plt.title('Loss vs Epoch')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "AH8_zNaf31z0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "outputId": "a5755934-fb70-4d2e-92eb-cd2328cc8482"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjcAAAHHCAYAAABDUnkqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjcuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/bCgiHAAAACXBIWXMAAA9hAAAPYQGoP6dpAAAuoElEQVR4nO3deXQUZb7/8U8nkE4ISbOZBQkQAdnDvgQQcUAiIhLhKjLMgPsVgxOGUX8TlBFQDOLguKABBpHrgmxHwMuIGEBhEJCwKaBGRSAREpARuiFggKR+f3jomb4hQKBJdT+8X+fUOVTVU9XfqtOkPuepp6odlmVZAgAAMESI3QUAAAD4E+EGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAAsw999yj6tWr210GELQIN8BVZM6cOXI4HNq8ebPdpdjqnnvukcPhOOcUHh5ud3kALlMVuwsAADs4nU7NmjWrzPLQ0FAbqgHgT4QbAFelKlWq6He/+53dZQC4ArgtBaCMbdu2qV+/foqOjlb16tXVu3dvbdy40afN6dOnNWHCBDVp0kTh4eGqXbu2evTooezsbG+bwsJC3XvvvapXr56cTqfi4+M1cOBA7d27t9zP/utf/yqHw6F9+/aVWZeRkaGwsDAdOXJEkvTdd99p8ODBiouLU3h4uOrVq6e7775bbrfbL+fh7G28tWvX6r//+79Vu3ZtRUdHa/jw4d4a/tPrr7+uli1byul0qm7dukpLS9PRo0fLtPv888916623qmbNmoqMjFRSUpJefvnlMu3279+v1NRUVa9eXddcc40ee+wxlZSU+OXYAJPRcwPAx65du3TDDTcoOjpaTzzxhKpWraoZM2aoV69eWrNmjbp06SJJGj9+vDIzM/XAAw+oc+fO8ng82rx5s7Zu3aqbb75ZkjR48GDt2rVLjz76qBo2bKhDhw4pOztbeXl5atiw4Tk//6677tITTzyhBQsW6PHHH/dZt2DBAvXt21c1a9bUqVOnlJKSouLiYj366KOKi4vT/v37tWzZMh09elQul+uCx3r48OEyy8LCwhQdHe2zbNSoUapRo4bGjx+v3NxcZWVlad++ffr000/lcDi852PChAnq06ePRo4c6W2Xk5Ojzz77TFWrVpUkZWdn67bbblN8fLzS09MVFxenr7/+WsuWLVN6err3M0tKSpSSkqIuXbror3/9q1auXKmpU6eqUaNGGjly5AWPDbiqWQCuGm+++aYlycrJySm3TWpqqhUWFmbt3r3bu+zAgQNWVFSU1bNnT++yNm3aWP379y93P0eOHLEkWS+88EKF60xOTrY6dOjgs2zTpk2WJOutt96yLMuytm3bZkmyFi5cWOH9jxgxwpJ0ziklJcXb7uz56tChg3Xq1Cnv8ilTpliSrKVLl1qWZVmHDh2ywsLCrL59+1olJSXedtOmTbMkWbNnz7Ysy7LOnDljJSYmWg0aNLCOHDniU1NpaWmZ+iZOnOjTpl27dmXOC4CyuC0FwKukpEQff/yxUlNTdd1113mXx8fH67e//a3WrVsnj8cjSapRo4Z27dql77777pz7ioiIUFhYmD799NNz3sI5nyFDhmjLli3avXu3d9n8+fPldDo1cOBASfL2zKxYsUInTpyo0P4lKTw8XNnZ2WWmyZMnl2n70EMPeXteJGnkyJGqUqWKPvzwQ0nSypUrderUKY0ePVohIf/+s/rggw8qOjpa//jHPyT9ertvz549Gj16tGrUqOHzGWd7gP7Tww8/7DN/ww036IcffqjwsQJXG8INAK+ffvpJJ06cUNOmTcusa968uUpLS5Wfny9Jmjhxoo4eParrr79erVu31uOPP64vv/zS297pdOr555/X8uXLFRsbq549e2rKlCkqLCy8YB133nmnQkJCNH/+fEmSZVlauHChdxyQJCUmJmrMmDGaNWuW6tSpo5SUFL322msXPd4mNDRUffr0KTO1bdu2TNsmTZr4zFevXl3x8fHesUNnxwf93/MWFham6667zrv+bFhr1arVBesLDw/XNddc47OsZs2aFQ6KwNWIcAPgkvTs2VO7d+/W7Nmz1apVK82aNUvt27f3ebx69OjR+vbbb5WZmanw8HCNGzdOzZs317Zt286777p16+qGG27QggULJEkbN25UXl6ehgwZ4tNu6tSp+vLLLzV27FidPHlSf/jDH9SyZUv9+OOP/j/gSsYj6cClI9wA8LrmmmtUrVo15ebmlln3zTffKCQkRAkJCd5ltWrV0r333qv33ntP+fn5SkpK0vjx4322a9Sokf70pz/p448/1s6dO3Xq1ClNnTr1grUMGTJEX3zxhXJzczV//nxVq1ZNAwYMKNOudevWeuqpp7R27Vr985//1P79+zV9+vSKH/x5/N9bb8ePH1dBQYF3UHSDBg0kqcx5O3XqlPbs2eNd36hRI0nSzp07/VofAF+EGwBeoaGh6tu3r5YuXerzuPbBgwc1d+5c9ejRw3tb6F//+pfPttWrV1fjxo1VXFwsSTpx4oR++eUXnzaNGjVSVFSUt835DB48WKGhoXrvvfe0cOFC3XbbbYqMjPSu93g8OnPmjM82rVu3VkhIyEXtvyJmzpyp06dPe+ezsrJ05swZ9evXT5LUp08fhYWF6ZVXXpFlWd52b7zxhtxut/r37y9Jat++vRITE/XSSy+VeUT8P7cDcHl4FBy4Cs2ePVsfffRRmeXp6el69tlnlZ2drR49euiRRx5RlSpVNGPGDBUXF2vKlCneti1atFCvXr3UoUMH1apVS5s3b9aiRYs0atQoSdK3336r3r1766677lKLFi1UpUoVLV68WAcPHtTdd999wRpjYmJ000036cUXX9SxY8fK3JJavXq1Ro0apTvvvFPXX3+9zpw5o7fffluhoaEaPHjwBfd/5swZvfPOO+dcd8cdd/gEqVOnTnmPJTc3V6+//rp69Oih22+/XdKvPV4ZGRmaMGGCbrnlFt1+++3edp06dfK+LDAkJERZWVkaMGCA2rZtq3vvvVfx8fH65ptvtGvXLq1YseKCdQO4CDY/rQWgEp19tLm8KT8/37Isy9q6dauVkpJiVa9e3apWrZp10003WevXr/fZ17PPPmt17tzZqlGjhhUREWE1a9bMmjRpkveR6cOHD1tpaWlWs2bNrMjISMvlclldunSxFixYcNH1/v3vf7ckWVFRUdbJkyd91v3www/WfffdZzVq1MgKDw+3atWqZd10003WypUrL7jf8z0KLsnas2ePz/las2aN9dBDD1k1a9a0qlevbg0bNsz617/+VWa/06ZNs5o1a2ZVrVrVio2NtUaOHFnmkW/Lsqx169ZZN998sxUVFWVFRkZaSUlJ1quvvupTX2RkZJntnn76aYs/28CFOSyLvlAAOJc5c+bo3nvvVU5Ojjp27Gh3OQAuEmNuAACAUQg3AADAKIQbAABgFMbcAAAAo9BzAwAAjEK4AQAARgmYl/hNnjxZGRkZSk9P10svvVRuu4ULF2rcuHHau3evmjRpoueff1633nrrRX9OaWmpDhw4oKioqHP+Ci8AAAg8lmXp2LFjqlu3rkJCzt83ExDhJicnRzNmzFBSUtJ5261fv15Dhw5VZmambrvtNs2dO1epqanaunXrRf3KriQdOHDA57dxAABA8MjPz1e9evXO28b2AcXHjx9X+/bt9frrr+vZZ59V27Zty+25GTJkiIqKirRs2TLvsq5du6pt27YX/UN5brdbNWrUUH5+vvc3cgAAQGDzeDxKSEjQ0aNH5XK5ztvW9p6btLQ09e/fX3369NGzzz573rYbNmzQmDFjfJalpKRoyZIl5W5TXFzs8yN6x44dkyRFR0cTbgAACDIXM6TE1nAzb948bd26VTk5ORfVvrCwULGxsT7LYmNjVVhYWO42mZmZmjBhwmXVCQAAgodtT0vl5+crPT1d7777rsLDw6/Y52RkZMjtdnun/Pz8K/ZZAADAfrb13GzZskWHDh1S+/btvctKSkq0du1aTZs2TcXFxQoNDfXZJi4uTgcPHvRZdvDgQcXFxZX7OU6nU06n07/FAwCAgGVbz03v3r21Y8cObd++3Tt17NhRw4YN0/bt28sEG0lKTk7WqlWrfJZlZ2crOTm5ssoGAAABzraem6ioqDKPb0dGRqp27dre5cOHD9e1116rzMxMSVJ6erpuvPFGTZ06Vf3799e8efO0efNmzZw5s9LrBwAAgSmg31Ccl5engoIC73y3bt00d+5czZw5U23atNGiRYu0ZMmSi37HDQAAMJ/t77mpbB6PRy6XS263m0fBAQAIEhW5fgd0zw0AAEBFEW4AAIBRCDcAAMAohBsAAGAUwo0fFbhPav3uwypwn7S7FAAArlq2/3CmKebn5Cnj/R0qtaQQh5Q5qLWGdKpvd1kAAFx16LnxgwL3SW+wkaRSSxr7/k56cAAAsAHhxg/2HC7yBpuzSixLew+fsKcgAACuYoQbP0isE6kQh++yUIdDDetUs6cgAACuYoQbP4h3RShzUGuFOn5NOKEOh54b1ErxrgibKwMA4OrDgGI/GdKpvnpef432Hj6hhnWqEWwAALAJ4caP4l0RhBoAAGzGbSkAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMYmu4ycrKUlJSkqKjoxUdHa3k5GQtX7683PZz5syRw+HwmcLDwyuxYgAAEOiq2Pnh9erV0+TJk9WkSRNZlqX/+Z//0cCBA7Vt2za1bNnynNtER0crNzfXO+9wOCqrXAAAEARsDTcDBgzwmZ80aZKysrK0cePGcsONw+FQXFxcZZQHAACCUMCMuSkpKdG8efNUVFSk5OTkctsdP35cDRo0UEJCggYOHKhdu3add7/FxcXyeDw+EwAAMJft4WbHjh2qXr26nE6nHn74YS1evFgtWrQ4Z9umTZtq9uzZWrp0qd555x2VlpaqW7du+vHHH8vdf2Zmplwul3dKSEi4UocCAAACgMOyLMvOAk6dOqW8vDy53W4tWrRIs2bN0po1a8oNOP/p9OnTat68uYYOHapnnnnmnG2Ki4tVXFzsnfd4PEpISJDb7VZ0dLTfjgMAAFw5Ho9HLpfroq7fto65kaSwsDA1btxYktShQwfl5OTo5Zdf1owZMy64bdWqVdWuXTt9//335bZxOp1yOp1+qxcAAAQ2229L/V+lpaU+PS3nU1JSoh07dig+Pv4KVwUAAIKFrT03GRkZ6tevn+rXr69jx45p7ty5+vTTT7VixQpJ0vDhw3XttdcqMzNTkjRx4kR17dpVjRs31tGjR/XCCy9o3759euCBB+w8DAAAEEBsDTeHDh3S8OHDVVBQIJfLpaSkJK1YsUI333yzJCkvL08hIf/uXDpy5IgefPBBFRYWqmbNmurQoYPWr19/UeNzAADA1cH2AcWVrSIDkgAAQGCoyPU74MbcAAAAXA7CDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKLaGm6ysLCUlJSk6OlrR0dFKTk7W8uXLz7vNwoUL1axZM4WHh6t169b68MMPK6laAAAQDGwNN/Xq1dPkyZO1ZcsWbd68Wb/5zW80cOBA7dq165zt169fr6FDh+r+++/Xtm3blJqaqtTUVO3cubOSKwcAAIHKYVmWZXcR/6lWrVp64YUXdP/995dZN2TIEBUVFWnZsmXeZV27dlXbtm01ffr0i9q/x+ORy+WS2+1WdHS03+oGAABXTkWu3wEz5qakpETz5s1TUVGRkpOTz9lmw4YN6tOnj8+ylJQUbdiwodz9FhcXy+Px+EwAAMBctoebHTt2qHr16nI6nXr44Ye1ePFitWjR4pxtCwsLFRsb67MsNjZWhYWF5e4/MzNTLpfLOyUkJPi1fgAAEFhsDzdNmzbV9u3b9fnnn2vkyJEaMWKEvvrqK7/tPyMjQ2632zvl5+f7bd8AACDwVLG7gLCwMDVu3FiS1KFDB+Xk5Ojll1/WjBkzyrSNi4vTwYMHfZYdPHhQcXFx5e7f6XTK6XT6t2gAABCwbO+5+b9KS0tVXFx8znXJyclatWqVz7Ls7Oxyx+gAAICrj609NxkZGerXr5/q16+vY8eOae7cufr000+1YsUKSdLw4cN17bXXKjMzU5KUnp6uG2+8UVOnTlX//v01b948bd68WTNnzrTzMAAAQACxNdwcOnRIw4cPV0FBgVwul5KSkrRixQrdfPPNkqS8vDyFhPy7c6lbt26aO3eunnrqKY0dO1ZNmjTRkiVL1KpVK7sOAQAABJiAe8/NlcZ7bgAACD5B+Z4bAAAAfyDcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABglEsKN/n5+frxxx+985s2bdLo0aM1c+ZMvxUGAABwKS4p3Pz2t7/VJ598IkkqLCzUzTffrE2bNunJJ5/UxIkT/VogAABARVxSuNm5c6c6d+4sSVqwYIFatWql9evX691339WcOXP8WR8AAECFXFK4OX36tJxOpyRp5cqVuv322yVJzZo1U0FBgf+qAwAAqKBLCjctW7bU9OnT9c9//lPZ2dm65ZZbJEkHDhxQ7dq1/VogAABARVxSuHn++ec1Y8YM9erVS0OHDlWbNm0kSR988IH3dhUAAIAdHJZlWZeyYUlJiTwej2rWrOldtnfvXlWrVk0xMTF+K9DfPB6PXC6X3G63oqOj7S4HAABchIpcvy+p5+bkyZMqLi72Bpt9+/bppZdeUm5ubkAHGwAAYL5LCjcDBw7UW2+9JUk6evSounTpoqlTpyo1NVVZWVl+LRAAAKAiLincbN26VTfccIMkadGiRYqNjdW+ffv01ltv6ZVXXvFrgQAAABVxSeHmxIkTioqKkiR9/PHHGjRokEJCQtS1a1ft27fPrwUCAABUxCWFm8aNG2vJkiXKz8/XihUr1LdvX0nSoUOHGKQLAABsdUnh5i9/+Ysee+wxNWzYUJ07d1ZycrKkX3tx2rVr59cCAQAAKuKSHwUvLCxUQUGB2rRpo5CQXzPSpk2bFB0drWbNmvm1SH/iUXAAAIJPRa7fVS71Q+Li4hQXF+f9dfB69erxAj8AAGC7S7otVVpaqokTJ8rlcqlBgwZq0KCBatSooWeeeUalpaX+rhEAAOCiXVLPzZNPPqk33nhDkydPVvfu3SVJ69at0/jx4/XLL79o0qRJfi0SAADgYl3SmJu6detq+vTp3l8DP2vp0qV65JFHtH//fr8V6G+MuQEAIPhc8Z9f+Pnnn885aLhZs2b6+eefL2WXAAAAfnFJ4aZNmzaaNm1ameXTpk1TUlLSZRcFAABwqS5pzM2UKVPUv39/rVy50vuOmw0bNig/P18ffvihXwsEAACoiEvqubnxxhv17bff6o477tDRo0d19OhRDRo0SLt27dLbb7/t7xoBAAAu2iW/xO9cvvjiC7Vv314lJSX+2qXfMaAYAIDgc8UHFAMAAAQqwg0AADAK4QYAABilQk9LDRo06Lzrjx49ejm1AAAAXLYK9dy4XK7zTg0aNNDw4cMven+ZmZnq1KmToqKiFBMTo9TUVOXm5p53mzlz5sjhcPhM4eHhFTkMAABgsAr13Lz55pt+/fA1a9YoLS1NnTp10pkzZzR27Fj17dtXX331lSIjI8vdLjo62icEORwOv9YFAACC1yW9xM9fPvroI5/5OXPmKCYmRlu2bFHPnj3L3c7hcCguLu5KlwcAAIJQQA0odrvdkqRatWqdt93x48fVoEEDJSQkaODAgdq1a1e5bYuLi+XxeHwmAABgroAJN6WlpRo9erS6d++uVq1alduuadOmmj17tpYuXap33nlHpaWl6tatm3788cdzts/MzPQZF5SQkHClDgEAAAQAv76h+HKMHDlSy5cv17p161SvXr2L3u706dNq3ry5hg4dqmeeeabM+uLiYhUXF3vnPR6PEhISeEMxAABBpCJvKLZ1zM1Zo0aN0rJly7R27doKBRtJqlq1qtq1a6fvv//+nOudTqecTqc/ygQAAEHA1ttSlmVp1KhRWrx4sVavXq3ExMQK76OkpEQ7duxQfHz8FagQAAAEG1t7btLS0jR37lwtXbpUUVFRKiwslPTr+3QiIiIkScOHD9e1116rzMxMSdLEiRPVtWtXNW7cWEePHtULL7ygffv26YEHHrDtOAAAQOCwNdxkZWVJknr16uWz/M0339Q999wjScrLy1NIyL87mI4cOaIHH3xQhYWFqlmzpjp06KD169erRYsWlVU2AAAIYAEzoLiyVGRAEgAACAwVuX4HzKPgAAAA/kC4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKISbq1yB+6TW7z6sAvdJu0sBAMAvqthdAOwzPydPGe/vUKklhTikzEGtNaRTfbvLAgDgstBzc5UqcJ/0BhtJKrWkse/vpAcHABD0bA03mZmZ6tSpk6KiohQTE6PU1FTl5uZecLuFCxeqWbNmCg8PV+vWrfXhhx9WQrVm2XO4yBtsziqxLO09fMKeggAA8BNbw82aNWuUlpamjRs3Kjs7W6dPn1bfvn1VVFRU7jbr16/X0KFDdf/992vbtm1KTU1Vamqqdu7cWYmVB7/EOpEKcfguC3U41LBONXsKAgDATxyWZVkXblY5fvrpJ8XExGjNmjXq2bPnOdsMGTJERUVFWrZsmXdZ165d1bZtW02fPv2Cn+HxeORyueR2uxUdHe232oPR/Jw8jX1/p0osS6EOh54b1IoxNwCAgFSR63dADSh2u92SpFq1apXbZsOGDRozZozPspSUFC1ZsuSc7YuLi1VcXOyd93g8l1+oIYZ0qq+e11+jvYdPqGGdaop3RdhdEgAAly1gBhSXlpZq9OjR6t69u1q1alVuu8LCQsXGxvosi42NVWFh4TnbZ2ZmyuVyeaeEhAS/1h3s4l0RSm5Um2ADADBGwISbtLQ07dy5U/PmzfPrfjMyMuR2u71Tfn6+X/cPAAACS0Dclho1apSWLVumtWvXql69eudtGxcXp4MHD/osO3jwoOLi4s7Z3ul0yul0+q1WAAAQ2GztubEsS6NGjdLixYu1evVqJSYmXnCb5ORkrVq1ymdZdna2kpOTr1SZAAAgiNjac5OWlqa5c+dq6dKlioqK8o6bcblcioj4dQzI8OHDde211yozM1OSlJ6erhtvvFFTp05V//79NW/ePG3evFkzZ8607TgAAEDgsLXnJisrS263W7169VJ8fLx3mj9/vrdNXl6eCgoKvPPdunXT3LlzNXPmTLVp00aLFi3SkiVLzjsIGQAAXD0C6j03lYH33AAAEHwqcv0OmKelAAAA/IFwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwSdAvdJrd99WAXuk3aXAgAIQFXsLgCoiPk5ecp4f4dKLSnEIWUOaq0hnerbXRYAIIDQc4OgUeA+6Q02klRqSWPf30kPDgDAB+EGQWPP4SJvsDmrxLK09/AJewoCAAQkwg2CRmKdSIU4fJeFOhxqWKeaPQUBAAIS4QZBI94VocxBrRXq+DXhhDocem5QK8W7ImyuDAAQSBhQjKAypFN99bz+Gu09fEIN61Qj2AAAyiDcIOjEuyIINQCAcnFbCgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAABgFMINAAAwCuEGAAAYhXADAACMQrgBAABGIdwAAACjEG4AAIBRCDcAAMAohBsAAGAUwg0AADAK4QYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBsA5FbhPav3uwypwn7S7FACokCp2FwAg8MzPyVPG+ztUakkhDilzUGsN6VTf7rIA4KLY2nOzdu1aDRgwQHXr1pXD4dCSJUvO2/7TTz+Vw+EoMxUWFlZOwcBVoMB90htsJKnUksa+v5MeHABBw9ZwU1RUpDZt2ui1116r0Ha5ubkqKCjwTjExMVeoQuDqs+dwkTfYnFViWdp7+IQ9BQFABdl6W6pfv37q169fhbeLiYlRjRo1/F8QACXWiVSIQz4BJ9ThUMM61ewr6iIVuE9qz+EiJdaJVLwrwu5yEGD4flw9gnLMTdu2bVVcXKxWrVpp/Pjx6t69e7lti4uLVVxc7J33eDyVUSIQtOJdEcoc1Fpj39+pEstSqMOh5wa1CviLAeOEcD58P64uQRVu4uPjNX36dHXs2FHFxcWaNWuWevXqpc8//1zt27c/5zaZmZmaMGFCJVcKBLchneqr5/XXaO/hE2pYp1rAB5vyxgn1vP6agK8dVx7fj6tPUIWbpk2bqmnTpt75bt26affu3frb3/6mt99++5zbZGRkaMyYMd55j8ejhISEK14rEOziXRFB84f/fOOEguUYcOXw/bj6BFW4OZfOnTtr3bp15a53Op1yOp2VWBFQFvf6r6xgHieEKy+Yvx/87bg0Qf8Sv+3btys+Pt7uMoByzc/JU/fJq/Xbv3+u7pNXa35Ont0lGefsOKFQh0OSgmacECpHsH4/+Ntx6WztuTl+/Li+//577/yePXu0fft21apVS/Xr11dGRob279+vt956S5L00ksvKTExUS1bttQvv/yiWbNmafXq1fr444/tOgTgvLjXX3mCbZwQKlewfT/423F5bA03mzdv1k033eSdPzs2ZsSIEZozZ44KCgqUl/fvpHrq1Cn96U9/0v79+1WtWjUlJSVp5cqVPvsAAgn3+itXMI0TQuULpu8Hfzsuj63hplevXrIsq9z1c+bM8Zl/4okn9MQTT1zhqgD/CeZ7/QDsw9+OyxP0Y26AQBas9/oB2Iu/HZfHYZ2v68RAHo9HLpdLbrdb0dHRdpeDq0SB+2TQ3OsHEDj42/FvFbl+B/2j4EAwCKZ7/QACB387Lg23pQAAgFEINwBgowL3Sa3ffVgF7pN2lwL4RSB8p7ktBQA2CdYfc+StuShPoHyn6bkBABuU95K2QO/B4a25KE8gfacJNwBgg/O9pC1QBdLFC4EnkL7ThBsAsMHZl7T9p0B/SVsgXbwQeALpO024AQAbBONL2gLp4oXAE0jfaV7iBwA2CraXtM3PydPY93eqxLK8F69gGASNynOlvtMVuX4TbgAAFRJsgQxm4A3FAIArhrfmItAx5gYAABiFcAMAAIxCuAEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARrnqflvq7O+EejwemysBAAAX6+x1+2J+7/uqCzfHjh2TJCUkJNhcCQAAqKhjx47J5XKdt43DupgIZJDS0lIdOHBAUVFRcjgcft23x+NRQkKC8vPzL/hz7Lh0nOfKwXmuHJznysO5rhxX6jxblqVjx46pbt26Cgk5/6iaq67nJiQkRPXq1buinxEdHc1/nErAea4cnOfKwXmuPJzrynElzvOFemzOYkAxAAAwCuEGAAAYhXDjR06nU08//bScTqfdpRiN81w5OM+Vg/NceTjXlSMQzvNVN6AYAACYjZ4bAABgFMINAAAwCuEGAAAYhXADAACMQrjxk9dee00NGzZUeHi4unTpok2bNtldknEyMzPVqVMnRUVFKSYmRqmpqcrNzbW7LKNNnjxZDodDo0ePtrsUI+3fv1+/+93vVLt2bUVERKh169bavHmz3WUZpaSkROPGjVNiYqIiIiLUqFEjPfPMMxf1+0Qo39q1azVgwADVrVtXDodDS5Ys8VlvWZb+8pe/KD4+XhEREerTp4++++67SquPcOMH8+fP15gxY/T0009r69atatOmjVJSUnTo0CG7SzPKmjVrlJaWpo0bNyo7O1unT59W3759VVRUZHdpRsrJydGMGTOUlJRkdylGOnLkiLp3766qVatq+fLl+uqrrzR16lTVrFnT7tKM8vzzzysrK0vTpk3T119/reeff15TpkzRq6++andpQa2oqEht2rTRa6+9ds71U6ZM0SuvvKLp06fr888/V2RkpFJSUvTLL79UToEWLlvnzp2ttLQ073xJSYlVt25dKzMz08aqzHfo0CFLkrVmzRq7SzHOsWPHrCZNmljZ2dnWjTfeaKWnp9tdknH+3//7f1aPHj3sLsN4/fv3t+677z6fZYMGDbKGDRtmU0XmkWQtXrzYO19aWmrFxcVZL7zwgnfZ0aNHLafTab333nuVUhM9N5fp1KlT2rJli/r06eNdFhISoj59+mjDhg02VmY+t9stSapVq5bNlZgnLS1N/fv39/lew78++OADdezYUXfeeadiYmLUrl07/f3vf7e7LON069ZNq1at0rfffitJ+uKLL7Ru3Tr169fP5srMtWfPHhUWFvr8/XC5XOrSpUulXRevuh/O9LfDhw+rpKREsbGxPstjY2P1zTff2FSV+UpLSzV69Gh1795drVq1srsco8ybN09bt25VTk6O3aUY7YcfflBWVpbGjBmjsWPHKicnR3/4wx8UFhamESNG2F2eMf785z/L4/GoWbNmCg0NVUlJiSZNmqRhw4bZXZqxCgsLJemc18Wz6640wg2CUlpamnbu3Kl169bZXYpR8vPzlZ6eruzsbIWHh9tdjtFKS0vVsWNHPffcc5Kkdu3aaefOnZo+fTrhxo8WLFigd999V3PnzlXLli21fft2jR49WnXr1uU8G4zbUpepTp06Cg0N1cGDB32WHzx4UHFxcTZVZbZRo0Zp2bJl+uSTT1SvXj27yzHKli1bdOjQIbVv315VqlRRlSpVtGbNGr3yyiuqUqWKSkpK7C7RGPHx8WrRooXPsubNmysvL8+misz0+OOP689//rPuvvtutW7dWr///e/1xz/+UZmZmXaXZqyz1z47r4uEm8sUFhamDh06aNWqVd5lpaWlWrVqlZKTk22szDyWZWnUqFFavHixVq9ercTERLtLMk7v3r21Y8cObd++3Tt17NhRw4YN0/bt2xUaGmp3icbo3r17mVcZfPvtt2rQoIFNFZnpxIkTCgnxvdSFhoaqtLTUporMl5iYqLi4OJ/rosfj0eeff15p10VuS/nBmDFjNGLECHXs2FGdO3fWSy+9pKKiIt177712l2aUtLQ0zZ07V0uXLlVUVJT33q3L5VJERITN1ZkhKiqqzBimyMhI1a5dm7FNfvbHP/5R3bp103PPPae77rpLmzZt0syZMzVz5ky7SzPKgAEDNGnSJNWvX18tW7bUtm3b9OKLL+q+++6zu7Sgdvz4cX3//ffe+T179mj79u2qVauW6tevr9GjR+vZZ59VkyZNlJiYqHHjxqlu3bpKTU2tnAIr5Zmsq8Crr75q1a9f3woLC7M6d+5sbdy40e6SjCPpnNObb75pd2lG41HwK+d///d/rVatWllOp9Nq1qyZNXPmTLtLMo7H47HS09Ot+vXrW+Hh4dZ1111nPfnkk1ZxcbHdpQW1Tz755Jx/j0eMGGFZ1q+Pg48bN86KjY21nE6n1bt3bys3N7fS6nNYFq9pBAAA5mDMDQAAMArhBgAAGIVwAwAAjEK4AQAARiHcAAAAoxBuAACAUQg3AADAKIQbAFc9h8OhJUuW2F0GAD8h3ACw1T333COHw1FmuuWWW+wuDUCQ4relANjulltu0ZtvvumzzOl02lQNgGBHzw0A2zmdTsXFxflMNWvWlPTrLaOsrCz169dPERERuu6667Ro0SKf7Xfs2KHf/OY3ioiIUO3atfXQQw/p+PHjPm1mz56tli1byul0Kj4+XqNGjfJZf/jwYd1xxx2qVq2amjRpog8++ODKHjSAK4ZwAyDgjRs3ToMHD9YXX3yhYcOG6e6779bXX38tSSoqKlJKSopq1qypnJwcLVy4UCtXrvQJL1lZWUpLS9NDDz2kHTt26IMPPlDjxo19PmPChAm666679OWXX+rWW2/VsGHD9PPPP1fqcQLwk0r7iU4AOIcRI0ZYoaGhVmRkpM80adIky7J+/TX4hx9+2GebLl26WCNHjrQsy7Jmzpxp1axZ0zp+/Lh3/T/+8Q8rJCTEKiwstCzLsurWrWs9+eST5dYgyXrqqae888ePH7ckWcuXL/fbcQKoPIy5AWC7m266SVlZWT7LatWq5f13cnKyz7rk5GRt375dkvT111+rTZs2ioyM9K7v3r27SktLlZubK4fDoQMHDqh3797nrSEpKcn778jISEVHR+vQoUOXekgAbES4AWC7yMjIMreJ/CUiIuKi2lWtWtVn3uFwqLS09EqUBOAKY8wNgIC3cePGMvPNmzeXJDVv3lxffPGFioqKvOs/++wzhYSEqGnTpoqKilLDhg21atWqSq0ZgH3ouQFgu+LiYhUWFvosq1KliurUqSNJWrhwoTp27KgePXro3Xff1aZNm/TGG29IkoYNG6ann35aI0aM0Pjx4/XTTz/p0Ucf1e9//3vFxsZKksaPH6+HH35YMTEx6tevn44dO6bPPvtMjz76aOUeKIBKQbgBYLuPPvpI8fHxPsuaNm2qb775RtKvTzLNmzdPjzzyiOLj4/Xee++pRYsWkqRq1appxYoVSk9PV6dOnVStWjUNHjxYL774ondfI0aM0C+//KK//e1veuyxx1SnTh3913/9V+UdIIBK5bAsy7K7CAAoj8Ph0OLFi5Wammp3KQCCBGNuAACAUQg3AADAKIy5ARDQuHMOoKLouQEAAEYh3AAAAKMQbgAAgFEINwAAwCiEGwAAYBTCDQAAMArhBgAAGIVwAwAAjEK4AQAARvn/6760LiMgtMgAAAAASUVORK5CYII=\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Validation\n",
        "*Note:*\n",
        "\n",
        "*1. Predicted mask files are stored in a new folder under the \"project3\" folder called \"test\".*\n",
        "\n",
        "*2. Pixels of masks displayed in the report have been multiplied by 255 to make the different classes visible. The code below labels each pixel between 0-6 in align with the project requirement and the input required by evalseg.m.*"
      ],
      "metadata": {
        "id": "FGxDb0Wn0-Qq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask_from_target(target_dict, output_path):\n",
        "    threshold = 0.75\n",
        "\n",
        "    # Filter predictions based on score threshold.\n",
        "    high_score_indices = target_dict[\"scores\"] > threshold\n",
        "    filtered_labels = target_dict[\"labels\"][high_score_indices].cpu().numpy()\n",
        "    filtered_masks = target_dict[\"masks\"][high_score_indices].cpu().numpy()\n",
        "\n",
        "    # To handle the edge-case where none of the masks cross the threshold when testing on unseen data,\n",
        "    # all available predicted masks are included.\n",
        "    if filtered_labels.size == 0:\n",
        "        filtered_labels = target_dict[\"labels\"].cpu().numpy()\n",
        "        filtered_masks = target_dict[\"masks\"].cpu().numpy()\n",
        "\n",
        "    # Create combined mask.\n",
        "    combined_mask = np.zeros_like(filtered_masks[0], dtype=np.uint8)\n",
        "    for label, mask in zip(filtered_labels, filtered_masks):\n",
        "        combined_mask[mask > 0] = label\n",
        "\n",
        "    # Save the mask image.\n",
        "    combined_mask_pil = Image.fromarray(combined_mask.squeeze(), mode='L')\n",
        "    combined_mask_pil.save(output_path)\n",
        "\n",
        "# Load the final stage of the trained model and set to evaluate.\n",
        "model_clothes.load_state_dict(torch.load(\"10.torch\")) #INITALLY SET TO 500\n",
        "model_clothes.to(device)\n",
        "model_clothes.eval()\n",
        "\n",
        "# Select 10% of images for validation.\n",
        "image_validation_files = all_image_files[300:360]\n",
        "\n",
        "# Iterate through each image.\n",
        "for image in image_validation_files:\n",
        "    images = cv2.imread(os.path.join(\"drive\", \"MyDrive\", \"project3\", \"images\", image))\n",
        "    images = torch.as_tensor(images, dtype=torch.float32) / 255.0\n",
        "    images = images.permute(2,0,1).unsqueeze(0)\n",
        "    images = images.to(device)\n",
        "\n",
        "    # Get the prediction output of the model.\n",
        "    with torch.no_grad():\n",
        "        pred = model_clothes(images)\n",
        "\n",
        "    # Get the image number and create the appropriate file name and output path.\n",
        "    img_number = image[:-4]\n",
        "    output_path = os.path.join(\"drive\", \"MyDrive\", \"project3\", \"validation\", img_number + \"_clothes.png\")\n",
        "    create_mask_from_target(pred[0], output_path)"
      ],
      "metadata": {
        "id": "A5CQjlEl4KCd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test\n",
        "*Note:*\n",
        "\n",
        "*1. Predicted mask files are stored in a new folder under the \"project3\" folder called \"test\".*\n",
        "\n",
        "*2. Pixels of masks displayed in the report have been multiplied by 255 to make the different classes visible. The code below labels each pixel between 0-6 in align with the project requirement and the input required by evalseg.m.*"
      ],
      "metadata": {
        "id": "s7FxpXje1CCc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_mask_from_target(target_dict, output_path):\n",
        "    threshold = 0.75\n",
        "\n",
        "    # Filter predictions based on score threshold.\n",
        "    high_score_indices = target_dict[\"scores\"] > threshold\n",
        "    filtered_labels = target_dict[\"labels\"][high_score_indices].cpu().numpy()\n",
        "    filtered_masks = target_dict[\"masks\"][high_score_indices].cpu().numpy()\n",
        "\n",
        "    # To handle the edge-case where none of the masks cross the threshold when testing on unseen data,\n",
        "    # all available predicted masks are included.\n",
        "    if filtered_labels.size == 0:\n",
        "        filtered_labels = target_dict[\"labels\"].cpu().numpy()\n",
        "        filtered_masks = target_dict[\"masks\"].cpu().numpy()\n",
        "\n",
        "    # Create combined mask.\n",
        "    combined_mask = np.zeros_like(filtered_masks[0], dtype=np.uint8)\n",
        "    for label, mask in zip(filtered_labels, filtered_masks):\n",
        "        combined_mask[mask > 0] = label\n",
        "\n",
        "    # Save the mask image.\n",
        "    combined_mask_pil = Image.fromarray(combined_mask.squeeze(), mode='L')\n",
        "    combined_mask_pil.save(output_path)\n",
        "\n",
        "# Load the final stage of the trained model and set to evaluate.\n",
        "model_clothes.load_state_dict(torch.load(\"10.torch\")) #INITALLY SET TO 500\n",
        "model_clothes.to(device)\n",
        "model_clothes.eval()\n",
        "\n",
        "# Select 40% of images for testing.\n",
        "image_validation_files = all_image_files[360:]\n",
        "\n",
        "# Iterate through each image.\n",
        "for image in image_validation_files:\n",
        "    images = cv2.imread(os.path.join(\"drive\", \"MyDrive\", \"project3\", \"images\", image))\n",
        "    images = torch.as_tensor(images, dtype=torch.float32) / 255.0\n",
        "    images = images.permute(2,0,1).unsqueeze(0)\n",
        "    images = images.to(device)\n",
        "\n",
        "    # Get the prediction output of the model.\n",
        "    with torch.no_grad():\n",
        "        pred = model_clothes(images)\n",
        "\n",
        "    # Get the image number and create the appropriate file name and output path.\n",
        "    img_number = image[:-4]\n",
        "    output_path = os.path.join(\"drive\", \"MyDrive\", \"project3\", \"test\", img_number + \"_clothes.png\")\n",
        "    create_mask_from_target(pred[0], output_path)"
      ],
      "metadata": {
        "id": "9ssZSfZtlAI5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## References\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "*Note: The mask R-CNN model provided in the PyTorch’s torchvision package was used in the code to build the model and described in detail in the report. References that have been included below of coding tutorials were mostly used to develop an understanding of the package and the model before adapting the information for the specific use of this project. Special focus was given to not copy any sections of code directly.*\n",
        "\n",
        "[1] G. Gkioxari, K. He, P. Dollar, and R. Girshick, \"Mask R-CNN,\" Facebook AI Research (FAIR), January 2018.\n",
        "\n",
        "[2] G. Patterson, J. Hays, L. Bourdev, L. Zitnick, M. R. Ronchi, M. Maire, R. Girshick, S. Belongie, T. Lin, P. Perona, D. Ramanan, P. Dollar, and Y. Cui, \"What is COCO?\", COCO, 2015. [Online]. Available: https://cocodataset.org/#home. [Accessed: April, 2024].\n",
        "\n",
        "[3] K. Yamaguchi, M. H. Kiapour, L. E. Ortiz, T. L. Berg. “Parsing Clothing in Fashion Photographs,” CVPR, 2012.\n",
        "\n",
        "[4] P. Potrimba, \"What is Mask R-CNN? The Ultimate Guide.\" Roboflow, 2023. [Online]. Available: https://blog.roboflow.com/mask-rcnn/. [Accessed: April, 2024].\n",
        "\n",
        "[5] PyTorch, \"maskrcnn_resnet50_fpn,\" PyTorch, 2017. [Online]. Available: https://pytorch.org/vision/main/models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html. [Accessed: April, 2024].\n",
        "\n",
        "[6] PyTorch, \"Visualization utilities,\" PyTorch, 2017. [Online]. Available: https://pytorch.org/vision/main/models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html. [Accessed: April, 2024].\n",
        "\n",
        "[7] PyTorch, \"TorchVision Object Detection Finetuning Tutorial,\" PyTorch, 2017. [Online]. Available: https://pytorch.org/vision/main/models/generated/torchvision.models.detection.maskrcnn_resnet50_fpn.html. [Accessed: April, 2024].\n",
        "\n",
        "[8] S. Eppel, \"Train Mask R-CNN Net for Object Detection in 60 Lines of Code,\" Towards Data Science, 2022. [Online]. Available: https://towardsdatascience.com/train-mask-rcnn-net-for-object-detection-in-60-lines-of-code-9b6bbff292c3. [Accessed: April, 2024].\n"
      ],
      "metadata": {
        "id": "GdLYhiquzPvz"
      }
    }
  ]
}